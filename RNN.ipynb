{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "acceptable-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,roc_auc_score,recall_score,confusion_matrix\n",
    "import eli5\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer                    \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Embedding, Conv1D, Dense, MaxPooling1D, Flatten, Dropout\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "burning-hungary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  \n",
       "0            politics      2  \n",
       "1                 nba     -4  \n",
       "2                 nfl      3  \n",
       "3  BlackPeopleTwitter     -8  \n",
       "4  MaddenUltimateTeam      6  "
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.dropna(subset = [\"comment\"], inplace = True)\n",
    "df = pd.read_csv(\"train-balanced.csv\")\n",
    "df.drop(['ups', 'downs', 'date', 'created_utc', 'parent_comment'], axis=1, inplace = True)\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df = pd.read_csv(\"train-balanced.csv\")\n",
    "df.dropna(subset = [\"comment\"], inplace = True)\n",
    "df.drop(['ups', 'downs'], axis=1, inplace = True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "complete-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, SpatialDropout1D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "steady-collar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.23760933,  6.46630023,  0.61106807],\n",
       "       [ 4.37799267,  5.07579862, 15.27670167],\n",
       "       [ 3.6728097 ,  4.89110073,  0.61106807],\n",
       "       ...,\n",
       "       [ 4.32070511,  3.95316495,  0.61106807],\n",
       "       [ 4.28832324,  8.61886491,  0.61106807],\n",
       "       [ 3.47101432,  6.03810955,  4.27747647]])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_author = np.asarray(df['author'].factorize()[0])\n",
    "encoded_subreddit = np.asarray(df['subreddit'].factorize()[0])\n",
    "author_reshaped = np.reshape(np.log(encoded_author+0.1)/np.log(10) - 1, (len(encoded_author), 1))\n",
    "subreddit_reshaped = np.reshape(np.log(encoded_subreddit+0.1), (len(encoded_subreddit), 1))\n",
    "score_reshaped = np.reshape(df['score'].values*30000/np.linalg.norm(df['score'].values), (len(df['score']), 1))\n",
    "other_features = np.hstack((author_reshaped, subreddit_reshaped, score_reshaped))\n",
    "xtrain, xval, ytrain, yval = train_test_split(other_features, df['label'], test_size=0.2)\n",
    "xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "eligible-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyper parameters\n",
    "batch_size = 64\n",
    "embedding_dims = 30 #Length of the token vectors\n",
    "epochs = 5\n",
    "max_sequence_length = 100 #max length of comment\n",
    "max_number_words = 100000 #max vocab size of tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "prime-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "parliamentary-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "TBtokenizer = TreebankWordTokenizer()\n",
    "    \n",
    "tokenized = []\n",
    "for comment in df['comment']:\n",
    "  temp = TBtokenizer.tokenize(comment)\n",
    "  tokenized.append([word for word in temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "respected-level",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 227320 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_number_words)\n",
    "tokenizer.fit_on_texts(tokenized)\n",
    "sequences = tokenizer.texts_to_sequences(tokenized)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_length = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "instances_pad = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "positive-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(\"glove.6B.50d.txt\", tokenizer.word_index, embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "average-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_pad_input = Input(shape=(max_sequence_length))\n",
    "mode = Embedding(vocab_length, embedding_dims, weights=[embedding_matrix], input_length=max_sequence_length)(instances_pad_input)\n",
    "mode = LSTM(64, recurrent_dropout=0.2, dropout=0.1, return_sequences=False)(mode)\n",
    "other_features_input = Input(shape=(3))\n",
    "concat = concatenate([mode, other_features_input], axis=1)\n",
    "mode = Dense(64, activation='relu')(mode)\n",
    "output = Dense(1, activation='sigmoid')(mode)\n",
    "\n",
    "model = Model([instances_pad_input, other_features_input], output)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "nearby-gateway",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Embedding(vocab_length, embedding_dims, weights=[embedding_matrix], input_length=max_sequence_length))\n",
    "#model2.add(SpatialDropout1D(0.4))\n",
    "model2.add(LSTM(64, recurrent_dropout=0.2, return_sequences=False))\n",
    "model2.add(Dense(64, activation = 'relu'))\n",
    "model2.add(Dense(1, activation = 'sigmoid'))\n",
    "model2.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "K.set_value(model2.optimizer.learning_rate, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "lucky-vessel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_96 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_69 (Embedding)        (None, 100, 30)      6819630     input_96[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_51 (LSTM)                  (None, 64)           24320       embedding_69[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_86 (Dense)                (None, 64)           4160        lstm_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_97 (InputLayer)           [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_87 (Dense)                (None, 1)            65          dense_86[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,848,175\n",
      "Trainable params: 6,848,175\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-traveler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  695/12635 [>.............................] - ETA: 2:09:52 - loss: 0.6932 - accuracy: 0.4999"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "xtrain2, xval2, ytrain2, yval2 = train_test_split(instances_pad, df['label'], test_size=0.2)\n",
    "callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "#history = model2.fit(xtrain2, ytrain2, epochs=20, validation_data=(xval2, yval2), batch_size=batch_size, callbacks=[callback], verbose = 1)\n",
    "history = model.fit([xtrain2, xtrain], ytrain2, epochs=10, validation_data=[xval2, yval2], batch_size=batch_size, callbacks=[callback])\n",
    "\n",
    "\n",
    "#plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(xval2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_pred, y_val):\n",
    "    y_pred2 = list(map(lambda x: 1 if x > 0.5 else 0, y_pred))\n",
    "    print(\"Accuracy:\",round(accuracy_score(y_val, y_pred2),4))\n",
    "    print('Precision:', round(precision_score(y_val, y_pred2),4))\n",
    "    print('F1:', round(f1_score(y_val,y_pred2),4))\n",
    "    print('AUC:',round(roc_auc_score(y_val, y_pred2),4))\n",
    "    print('Recall: ', round(recall_score(y_val, y_pred2), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_pred, yval2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-boost",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
